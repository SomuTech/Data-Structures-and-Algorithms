"""Credit card Fraud detection"""
#import modules
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

data = pd.read_csv("/content/drive/MyDrive/ML Code & Datasets/creditcard.csv")
# differs based on dataset path
print("Column names : " , list(data))
print("Total columns : " , len(list(data)))

n_genuine = len(data[data["Class"] == 0])
n_fraud = len(data[data["Class"] == 1])
print("Number of genuine transactions : " , n_genuine)
print("Number of fraudulent transactions : " , n_fraud)

plt.pie([n_genuine, n_fraud] , labels=['Genuine', 'Fraud'], radius=1)
plt.show()

X, y = data.iloc[:,:-1], data.iloc[:,-1]
k_best = SelectKBest(f_classif, k=10)
k_best.fit(X,y)
# Mask returns a boolean array which has value 1 for every selected feature and 
#value 0 for not selected ones
mask = k_best.get_support()
not_mask = np.logical_not(mask)

all_features = np.array(list(X))

# separating the best 10 features onbtained from bad ones
best_features = all_features[mask]
bad_features = all_features[not_mask]

print('Best features : ', best_features)
print('Bad features : ', bad_features)

# Dropping the bad features from the features dataframe X 
X = X.drop(bad_features, axis=1)
X.head()
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

# Creating a Gaussian Naive Bayes object
nb = GaussianNB()
# cv_results stores the train score and estimator 
cv_results = cross_validate(nb, x_train, y_train, cv=10, scoring='recall',return_train_score = True)
print("Training scores from each fold : ", cv_results['train_score'])
# iteration with maximum train_score is selected for training model
max_score_index = np.argmax(cv_results['train_score'])
nb1=GaussianNB()
nb1.fit(x_train,y_train)
nb1.score(x_train,y_train)
y_pred=nb1.predict(x_test)

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
y_pred_train = nb1.predict(x_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))
# print the scores on training and test set

print('Training set score: {:.4f}'.format(nb1.score(x_train, y_train)))

print('Test set score: {:.4f}'.format(nb1.score(x_test, y_test)))

cm = confusion_matrix(y_test, y_pred)

print('Confusion matrix\n\n', cm)
print('\nTrue Positives(TP) = ', cm[0,0])
print('\nTrue Negatives(TN) = ', cm[1,1])
print('\nFalse Positives(FP) = ', cm[0,1])
print('\nFalse Negatives(FN) = ', cm[1,0])

"""
OUTPUT:

Column names :  ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',
 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 
'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']
Total columns :  31
Number of genuine transactions :  284315
Number of fraudulent transactions :  492

Best features :  ['V3' 'V4' 'V7' 'V10' 'V11' 'V12' 'V14' 'V16' 'V17' 'V18']
Bad features :  ['Time' 'V1' 'V2' 'V5' 'V6' 'V8' 'V9' 'V13' 'V15' 'V19' 'V20' 'V21' 'V22'
 'V23' 'V24' 'V25' 'V26' 'V27' 'V28' 'Amount']

Model accuracy score: 0.9910
Training-set accuracy score: 0.9908
Training set score: 0.9908
Test set score: 0.9910

Confusion matrix

 [[56378   498]
 [   14    72]]

True Positives(TP) =  56378
True Negatives(TN) =  72
False Positives(FP) =  498
False Negatives(FN) =  14

"""